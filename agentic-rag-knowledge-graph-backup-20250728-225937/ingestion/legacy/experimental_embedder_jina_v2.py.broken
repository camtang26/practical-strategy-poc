"""
Optimized embedder for Jina API with dynamic batching and concurrent processing.
"""

import httpx
import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv
import time
from collections import defaultdict

from .chunker import DocumentChunk

load_dotenv()

logger = logging.getLogger(__name__)


class OptimizedJinaEmbeddingGenerator:
    """Optimized embedding generator with dynamic batching and concurrent processing."""
    
    def __init__(
        self,
        model: str = "jina-embeddings-v4",
        base_batch_size: int = 100,
        max_concurrent_requests: int = 3,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        rate_limit_per_minute: int = 60
    ):
        """Initialize optimized Jina embedding generator."""
        self.model = model
        self.base_batch_size = base_batch_size
        self.max_concurrent_requests = max_concurrent_requests
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.rate_limit_per_minute = rate_limit_per_minute
        self.api_key = os.getenv('EMBEDDING_API_KEY')
        self.base_url = "https://api.jina.ai/v1"
        
        # Jina v4 specifications
        self.dimensions = 2048
        self.max_tokens = 8191
        self.max_chars_per_text = self.max_tokens * 4  # ~32k chars
        
        # Dynamic batching parameters
        self.min_batch_size = 10
        self.max_batch_size = 200
        
        # Rate limiting
        self.request_times = []
        self.semaphore = asyncio.Semaphore(max_concurrent_requests)
        
        # Performance tracking
        self.stats = defaultdict(lambda: {'count': 0, 'total_time': 0, 'errors': 0})
        
        # Connection pooling - create reusable client
        self._client = None
        self._client_lock = asyncio.Lock()
        
        if not self.api_key:
            raise ValueError("EMBEDDING_API_KEY not found in environment")

    async def _get_client(self):
        """Get or create the shared httpx client."""
        if self._client is None:
            async with self._client_lock:
                # Double-check after acquiring lock
                if self._client is None:
                    self._client = httpx.AsyncClient(
                        timeout=httpx.Timeout(30.0),
                        limits=httpx.Limits(max_keepalive_connections=10, max_connections=20)
                    )
        return self._client
    
    async def close(self):
        """Close the httpx client and clean up resources."""
        if self._client:
            await self._client.aclose()
            self._client = None
    
    def _calculate_dynamic_batch_size(self, texts: List[str]) -> int:
        """Calculate optimal batch size based on text characteristics."""
        if not texts:
            return self.base_batch_size
        
        # Calculate average text length
        avg_length = sum(len(t) for t in texts) / len(texts)
        
        # Adjust batch size based on average length
        if avg_length < 500:  # Short texts
            batch_size = min(self.max_batch_size, self.base_batch_size * 2)
        elif avg_length < 2000:  # Medium texts
            batch_size = self.base_batch_size
        else:  # Long texts
            batch_size = max(self.min_batch_size, self.base_batch_size // 2)
        
        # Consider rate limiting
        current_time = time.time()
        recent_requests = [t for t in self.request_times if current_time - t < 60]
        
        if len(recent_requests) > self.rate_limit_per_minute * 0.8:
            # Slow down if approaching rate limit
            batch_size = max(self.min_batch_size, batch_size // 2)
        
        return int(batch_size)
    
    async def _wait_for_rate_limit(self):
        """Implement rate limiting with exponential backoff."""
        current_time = time.time()
        self.request_times = [t for t in self.request_times if current_time - t < 60]
        
        if len(self.request_times) >= self.rate_limit_per_minute:
            # Calculate wait time
            oldest_request = min(self.request_times)
            wait_time = 60 - (current_time - oldest_request) + 1
            logger.info(f"Rate limit reached, waiting {wait_time:.1f}s")
            await asyncio.sleep(wait_time)
        
        self.request_times.append(current_time)
    
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text."""
        embeddings = await self.generate_embeddings_batch([text])
        return embeddings[0] if embeddings else []
    
    async def _make_api_request(
        self,
        texts: List[str],
        batch_id: int
    ) -> Tuple[List[List[float]], float]:
        """Make a single API request with retry logic."""
        start_time = time.time()
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        request_data = {
            "model": self.model,
            "input": texts
        }
        
        for attempt in range(self.max_retries):
            try:
                await self._wait_for_rate_limit()
                
                async with self.semaphore:  # Limit concurrent requests
                    client = await self._get_client()
                    response = await client.post(
                        f"{self.base_url}/embeddings",
                        headers=headers,
                        json=request_data
                    )
                        
                    if response.status_code == 200:
                        data = response.json()
                        elapsed = time.time() - start_time
                        self.stats[batch_id]['count'] += len(texts)
                        self.stats[batch_id]['total_time'] += elapsed
                        return [item["embedding"] for item in data["data"]], elapsed
                    
                    elif response.status_code == 429:  # Rate limit
                        retry_after = int(response.headers.get('Retry-After', 60))
                        logger.warning(f"Rate limited, waiting {retry_after}s")
                        await asyncio.sleep(retry_after)
                        continue
                    
                    else:
                        error_msg = f"Jina API error: {response.status_code} - {response.text}"
                        logger.error(error_msg)
                            if attempt == self.max_retries - 1:
                                self.stats[batch_id]['errors'] += 1
                                raise Exception(error_msg)
                        
            except Exception as e:
                if attempt == self.max_retries - 1:
                    logger.error(f"Failed to embed batch {batch_id} after {self.max_retries} attempts: {e}")
                    self.stats[batch_id]['errors'] += 1
                    raise
                
                wait_time = self.retry_delay * (2 ** attempt)
                logger.warning(f"Batch {batch_id} attempt {attempt + 1} failed: {e}, waiting {wait_time}s")
                await asyncio.sleep(wait_time)
        
        return [], 0
    
    async def generate_embeddings_batch(
        self,
        texts: List[str]
    ) -> List[List[float]]:
        """Generate embeddings for a batch of texts with dynamic sizing."""
        if not texts:
            return []
        
        # Process and validate texts
        processed_texts = []
        for text in texts:
            if not text or not text.strip():
                processed_texts.append("")
                continue
            
            # Truncate if too long
            if len(text) > self.max_chars_per_text:
                text = text[:self.max_chars_per_text]
                logger.warning(f"Truncated text from {len(text)} to {self.max_chars_per_text} chars")
            
            processed_texts.append(text)
        
        # Single batch if small enough
        if len(processed_texts) <= self.base_batch_size:
            embeddings, _ = await self._make_api_request(processed_texts, 0)
            return embeddings
        
        # Otherwise, process with dynamic batching
        batch_size = self._calculate_dynamic_batch_size(processed_texts)
        all_embeddings = [None] * len(processed_texts)
        
        # Create batches
        tasks = []
        for i in range(0, len(processed_texts), batch_size):
            batch_texts = processed_texts[i:i + batch_size]
            batch_indices = list(range(i, min(i + batch_size, len(processed_texts))))
            task = self._process_batch_with_indices(batch_texts, batch_indices, i // batch_size)
            tasks.append(task)
        
        # Process batches concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Collect results
        for batch_result in results:
            if isinstance(batch_result, Exception):
                logger.error(f"Batch processing failed: {batch_result}")
                continue
            
            embeddings, indices = batch_result
            for embedding, idx in zip(embeddings, indices):
                all_embeddings[idx] = embedding
        
        # Fill in any missing embeddings with zeros
        for i, embedding in enumerate(all_embeddings):
            if embedding is None:
                logger.warning(f"Failed to generate embedding for text {i}, using zero vector")
                all_embeddings[i] = [0.0] * self.dimensions
        
        return all_embeddings
    
    async def _process_batch_with_indices(
        self,
        texts: List[str],
        indices: List[int],
        batch_id: int
    ) -> Tuple[List[List[float]], List[int]]:
        """Process a batch and return embeddings with their original indices."""
        embeddings, _ = await self._make_api_request(texts, batch_id)
        return embeddings, indices
    
    async def embed_chunks(
        self,
        chunks: List[DocumentChunk],
        progress_callback: Optional[callable] = None
    ) -> List[DocumentChunk]:
        """Embed document chunks with optimized batching and progress tracking."""
        if not chunks:
            return []
        
        start_time = time.time()
        total_chunks = len(chunks)
        logger.info(f"Starting embedding generation for {total_chunks} chunks")
        
        # Extract texts
        texts = [chunk.content for chunk in chunks]
        
        # Generate embeddings with progress tracking
        embeddings = await self._embed_with_progress(texts, progress_callback, total_chunks)
        
        # Create embedded chunks
        embedded_chunks = []
        for chunk, embedding in zip(chunks, embeddings):
            embedded_chunk = DocumentChunk(
                content=chunk.content,
                index=chunk.index,
                start_char=chunk.start_char,
                end_char=chunk.end_char,
                metadata={
                    **chunk.metadata,
                    "embedding_model": self.model,
                    "embedding_generated_at": datetime.now().isoformat(),
                    "embedding_dimensions": self.dimensions
                },
                token_count=chunk.token_count
            )
            embedded_chunk.embedding = embedding
            embedded_chunks.append(embedded_chunk)
        
        # Log performance stats
        elapsed = time.time() - start_time
        chunks_per_second = total_chunks / elapsed if elapsed > 0 else 0
        logger.info(f"Completed embedding generation: {total_chunks} chunks in {elapsed:.1f}s ({chunks_per_second:.1f} chunks/s)")
        
        return embedded_chunks
    
    async def _embed_with_progress(
        self,
        texts: List[str],
        progress_callback: Optional[callable],
        total_items: int
    ) -> List[List[float]]:
        """Embed texts with progress tracking and ETA calculation."""
        batch_size = self._calculate_dynamic_batch_size(texts)
        total_batches = (len(texts) + batch_size - 1) // batch_size
        completed_items = 0
        start_time = time.time()
        
        all_embeddings = []
        
        for batch_num in range(total_batches):
            batch_start = batch_num * batch_size
            batch_end = min((batch_num + 1) * batch_size, len(texts))
            batch_texts = texts[batch_start:batch_end]
            
            # Generate embeddings for batch
            batch_embeddings = await self.generate_embeddings_batch(batch_texts)
            all_embeddings.extend(batch_embeddings)
            
            # Update progress
            completed_items += len(batch_texts)
            if progress_callback:
                elapsed = time.time() - start_time
                items_per_second = completed_items / elapsed if elapsed > 0 else 0
                eta_seconds = (total_items - completed_items) / items_per_second if items_per_second > 0 else 0
                eta = timedelta(seconds=int(eta_seconds))
                
                progress_data = {
                    'current_batch': batch_num + 1,
                    'total_batches': total_batches,
                    'completed_items': completed_items,
                    'total_items': total_items,
                    'percentage': (completed_items / total_items) * 100,
                    'items_per_second': items_per_second,
                    'eta': str(eta)
                }
                progress_callback(progress_data)
        
        return all_embeddings
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get performance statistics."""
        total_items = sum(s['count'] for s in self.stats.values())
        total_time = sum(s['total_time'] for s in self.stats.values())
        total_errors = sum(s['errors'] for s in self.stats.values())
        
        return {
            'total_items_processed': total_items,
            'total_time_seconds': total_time,
            'average_time_per_item': total_time / total_items if total_items > 0 else 0,
            'total_errors': total_errors,
            'error_rate': total_errors / total_items if total_items > 0 else 0
        }


def create_optimized_embedder(**kwargs) -> OptimizedJinaEmbeddingGenerator:
    """Create an optimized embedding generator instance."""
    model = os.getenv('EMBEDDING_MODEL', 'jina-embeddings-v4')
    
    # Use optimized generator for Jina
    if 'jina' in model.lower():
        return OptimizedJinaEmbeddingGenerator(model=model, **kwargs)
    else:
        # Fall back to original embedder for other models
        from .embedder import EmbeddingGenerator
        return EmbeddingGenerator(model=model, **kwargs)
